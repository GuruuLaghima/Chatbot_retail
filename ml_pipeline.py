import json
import re
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import random
from difflib import get_close_matches
from collections import Counter

# ML Libraries 
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier


# NLP 
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

@dataclass
class PredictionResult:
    """Structure pour les r√©sultats de pr√©diction"""
    intent: str
    confidence: float
    entities: Dict[str, str]
    response_template: str

class OptimizedTextPreprocessor:
    """Preprocessing ultra-optimis√© pour le fran√ßais"""
    
    def __init__(self):
        self.french_stopwords = set(stopwords.words('french'))
        
        # Stopwords √† ne pas filtrer pour la mode
        self.preserve_words = {
            'sac', 'sur', 'en', 'de', 'du', 'le', 'la', 'un', 'une',
            'pour', 'avec', 'sans', 'tr√®s', 'bien', 'bon', 'belle'
        }
        
        # Filtrage intelligent des stopwords
        self.filtered_stopwords = self.french_stopwords - self.preserve_words
        
        # Mots fashion cruciaux √† pr√©server
        self.fashion_keywords = {
            # Produits
            'tailleur', 'robe', 'sac', 'sacoche', 'maroquinerie',
            'chaussure', 'chaussures', 'escarpin', 'escarpins', 'ballerine', 'ballerines',
            'manteau', 'veste', 'bijou', 'bijoux', 'collier', 'bracelet',
            
            # Mati√®res
            'tweed', 'cuir', 'agneau', 'soie', 'satin', 'matelasse', 'boucle',
            'laine', 'coton', 'perle', 'perles', 'metal',
            
            # Couleurs
            'noir', 'blanc', 'rouge', 'beige', 'marine', 'camel', 'gris', 'rose',
            'vert', 'bleu', 'dore', 'argente', 'multicolore', 'nude',
            
            # Occasions
            'business', 'bureau', 'travail', 'professionnel',
            'soiree', 'cocktail', 'gala', 'mariage', 'ceremonie',
            'quotidien', 'decontracte', 'casual', 'ville',
            
            # Actions/Verbes fashion
            'cherche', 'chercher', 'voudrais', 'veux', 'besoin',
            'recommande', 'conseille', 'suggere',
            'disponible', 'stock', 'inventaire',
            'prix', 'cout', 'tarif', 'budget',
            'entretien', 'nettoyer', 'soin', 'maintenance',
            
            # Marque/Style
            'chanel', 'heritage', 'maison', 'collection',
            'elegant', 'moderne', 'classique', 'iconique',
            'luxe', 'haut', 'gamme', 'prestige'
        }
        
        # Patterns d'entit√©s am√©lior√©s
        self.entity_patterns = {
            'size': r'\b(3[4-8]|4[0-6]|small|medium|large|s|m|l|xl|petite?|grande?|taille)\b',
            'color': r'\b(noir|blanc|rouge|beige|marine|camel|rose|gris|bleu|vert|dore|argente|multicolore|nude)\w*\b',
            'price': r'\b(\d+[‚Ç¨$]|\d+\s*euros?|prix|cout|tarif|budget|montant)\b',
            'product_id': r'\b(CMH\d{3})\b',
            'occasion': r'\b(bureau|soiree|cocktail|mariage|business|quotidien|gala|decontracte|travail|ceremonie)\b',
            'material': r'\b(cuir|tweed|soie|satin|laine|agneau|perles?|metal|coton|matelasse)\b',
            'brand': r'\b(chanel|heritage|maison)\b',
            'location': r'\b(paris|lyon|cannes|france|magasin|boutique|entrepot)\b',
            'product_type': r'\b(sac|chaussure|robe|tailleur|manteau|veste|bijou|collier|escarpin|ballerine)\w*\b'
        }

        self.vocabulary = []  
        self.vocab_built = False  

    def build_vocabulary_from_data(self, training_data: List[Dict]):
        """Construit automatiquement le vocabulaire depuis les donn√©es"""
        if self.vocab_built:
            return
            
        all_words = []
        
        # Extraire tous les mots de vos donn√©es d'entra√Ænement
        for example in training_data:
            text = example.get('text', '')
            # Preprocessing l√©ger pour extraire mots propres
            clean_text = self.clean_and_normalize(text)
            words = clean_text.split()
            all_words.extend(words)
        
        # Ajouter mots de vos patterns d'entit√©s existants
        for pattern in self.entity_patterns.values():
            # Extraire mots des regex (simplification)
            import re
            words_in_pattern = re.findall(r'\b[a-zA-Z]{3,}\b', pattern)
            all_words.extend(words_in_pattern)
        
        # Ajouter mots de vos fashion_keywords existants
        all_words.extend(self.fashion_keywords)
        
        # Garder seulement les mots fr√©quents (filtre automatique)
        word_counts = Counter(all_words)
        self.vocabulary = [word for word, count in word_counts.items() if count >= 2]
        
        self.vocab_built = True
        print(f"‚úÖ Vocabulaire automatique construit: {len(self.vocabulary)} mots")
    
    def auto_correct_typos(self, text: str) -> str:
        """Correction automatique sans dictionnaire pr√©d√©fini"""
        if not self.vocab_built or not self.vocabulary:
            return text
        
        words = text.lower().split()
        corrected_words = []
        
        for word in words:
            # Seulement corriger si le mot fait 4+ caract√®res
            # (√©vite de corriger "sac" ‚Üí "sec")
            if len(word) >= 4:
                # Chercher correspondance proche dans VOTRE vocabulaire
                matches = get_close_matches(
                    word, 
                    self.vocabulary, 
                    n=1,           
                    cutoff=0.8     # 80% de similarit√© minimum
                )
                
                if matches and matches[0] != word:
                    # Seulement si diff√©rent ET plus proche
                    corrected_words.append(matches[0])
                else:
                    corrected_words.append(word)
            else:
                # Mots courts : pas de correction
                corrected_words.append(word)
        
        return ' '.join(corrected_words)
    
    def clean_and_normalize(self, text: str) -> str:
        """Nettoyage optimis√©"""
        if not text:
            return ""
        
        text = text.lower()
        
        #  Normalisation fran√ßaise simplifi√©e
        replacements = {
            '√†': 'a', '√¢': 'a', '√§': 'a', '√°': 'a',
            '√©': 'e', '√®': 'e', '√™': 'e', '√´': 'e',
            '√Æ': 'i', '√Ø': 'i', '√≠': 'i',
            '√¥': 'o', '√∂': 'o', '√≥': 'o',
            '√π': 'u', '√ª': 'u', '√º': 'u', '√∫': 'u',
            '√ß': 'c', '√±': 'n'
        }
        
        for accented, clean in replacements.items():
            text = text.replace(accented, clean)
        
        # Nettoyage doux
        text = re.sub(r'[^\w\s\'-]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Extraction d'entit√©s robuste"""
        entities = {}
        text_clean = self.clean_and_normalize(text)
        
        for entity_type, pattern in self.entity_patterns.items():
            matches = re.findall(pattern, text_clean, re.IGNORECASE)
            if matches:
                # D√©duplication et nettoyage
                unique_matches = list(set(match.strip() for match in matches if match.strip()))
                if unique_matches:
                    entities[entity_type] = unique_matches
        
        return entities
    
    def intelligent_tokenize(self, text: str) -> List[str]:
        """Tokenisation intelligente """
        clean_text = self.clean_and_normalize(text)
        tokens = clean_text.split()
        
        filtered_tokens = []
        for token in tokens:
            if token in self.fashion_keywords:
                # Garde ABSOLUMENT les mots fashion
                filtered_tokens.append(token)
            elif token in self.preserve_words:
                # Garde les mots courts importants
                filtered_tokens.append(token)
            elif token not in self.filtered_stopwords and len(token) >= 2:
                #  Seuil r√©duit √† 2 caract√®res
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def preprocess_for_ml(self, text: str) -> str:
        """Preprocessing final pour ML"""

        text = self.auto_correct_typos(text)
        tokens = self.intelligent_tokenize(text)
        return ' '.join(tokens)

class OptimizedIntentClassifier:
    """Classificateur"""
    
    def __init__(self):
        self.preprocessor = OptimizedTextPreprocessor()
        
        # TF-IDF optimis√© pour petit dataset
        self.vectorizer = TfidfVectorizer(
            max_features=1500,      
            ngram_range=(1, 2),     
            min_df=1,               
            max_df=0.85,            
            analyzer='word',
            lowercase=True,
            strip_accents='unicode',
            token_pattern=r'\b[a-zA-Z]{2,}\b',
            sublinear_tf=True
        )
        
        # Mod√®les mieux configur√©s
        self.models = {
            'logistic': LogisticRegression(
                C=0.5,                   
                max_iter=1000,
                class_weight='balanced',
                random_state=42,
                solver='liblinear'        
            ),
            'svm_linear': SVC(
                kernel='linear',
                C=0.8,                    
                probability=True,
                class_weight='balanced',
                random_state=42
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=150,        
                max_depth=8,              
                min_samples_split=3,     
                min_samples_leaf=2,
                class_weight='balanced',
                random_state=42
            ),
            
            'xgboost': XGBClassifier(
                n_estimators=100,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                eval_metric='mlogloss'  
            ),
            'lightgbm': LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbose=-1  
            )
        }
        
        self.best_model = None
        self.best_model_name = None
        self.label_encoder = LabelEncoder()
        self.is_trained = False
   
    
    def prepare_data(self, training_data: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
        """Pr√©paration robuste des donn√©es"""
        texts = []
        labels = []
        
        for example in training_data:
            if 'text' in example and 'intent' in example:
                processed_text = self.preprocessor.preprocess_for_ml(example['text'])
                if processed_text.strip(): 
                    texts.append(processed_text)
                    labels.append(example['intent'])
        
        return np.array(texts), np.array(labels)
    
    def train(self, training_data: List[Dict]) -> Dict[str, float]:
        """Entra√Ænement robuste avec validation crois√©e"""
        print("ü§ñ Entra√Ænement optimis√© du classificateur...")
        
        # Pr√©paration des donn√©es
        X_text, y = self.prepare_data(training_data)
        
        print(f"üìä Donn√©es: {len(X_text)} exemples, {len(set(y))} classes")
        print(f"üìä Classes: {list(set(y))}")
        
        #V√©rification des donn√©es
        if len(X_text) == 0:
            raise ValueError("Aucune donn√©e valide apr√®s preprocessing!")
        
        # Encodage des labels
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Vectorisation
        X_vectorized = self.vectorizer.fit_transform(X_text)
        print(f"üìä Features TF-IDF: {X_vectorized.shape}")
        
        # Validation crois√©e pour s√©lection robuste
        model_scores = {}
        model_cv_scores = {}
        trained_models = {}
        
        for model_name, model in self.models.items():
            print(f"   üî¨ Test {model_name} avec validation crois√©e...")
            
            # Validation crois√©e stratifi√©e
            try:
                cv_scores = cross_val_score(
                    model, X_vectorized, y_encoded, 
                    cv=3, scoring='accuracy', n_jobs=-1
                )
                mean_cv_score = cv_scores.mean()
                model_cv_scores[model_name] = mean_cv_score
                
                # Entra√Ænement sur tout le dataset
                model.fit(X_vectorized, y_encoded)
                trained_models[model_name] = model
                
                print(f"      ‚úÖ {model_name}: CV={mean_cv_score:.3f} (¬±{cv_scores.std():.3f})")
                
            except Exception as e:
                print(f"      ‚ùå {model_name}: Erreur - {e}")
                model_cv_scores[model_name] = 0.0
        
        # S√©lection bas√©e sur validation crois√©e
        if model_cv_scores:
            self.best_model_name = max(model_cv_scores, key=model_cv_scores.get)
            self.best_model = trained_models[self.best_model_name]
            best_score = model_cv_scores[self.best_model_name]
        else:
            raise ValueError("Aucun mod√®le n'a pu √™tre entra√Æn√©!")
        
        print(f"üèÜ Meilleur: {self.best_model_name} (CV: {best_score:.3f})")

        # üî• AJOUTEZ ICI LE CODE DE TEST
        print("üß™ Test train/test split pour comparaison...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_vectorized, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        # Cr√©er une nouvelle instance du meilleur mod√®le pour le test
        test_model = trained_models[self.best_model_name].__class__(**trained_models[self.best_model_name].get_params())
        test_model.fit(X_train, y_train)
        simple_accuracy = test_model.score(X_test, y_test)
        print(f"üß™ Test simple (train/test): {simple_accuracy:.3f}")
        print(f"üìä Comparaison: CV={best_score:.3f} vs Train/Test={simple_accuracy:.3f}")
        
        self.is_trained = True
        
        return {
            'best_model': self.best_model_name,
            'cv_accuracy': best_score,
            'train_test_accuracy': simple_accuracy, 
            'all_cv_scores': model_cv_scores
        }
        
        self.is_trained = True
        
        return {
            'best_model': self.best_model_name,
            'cv_accuracy': best_score,
            'all_cv_scores': model_cv_scores
        }
    
    def predict(self, text: str) -> PredictionResult:
        """Pr√©diction robuste"""
        if not self.is_trained:
            raise ValueError("Mod√®le non entra√Æn√©!")
        
        # Preprocessing
        processed_text = self.preprocessor.preprocess_for_ml(text)
        
        # Gestion des textes vides
        if not processed_text.strip():
            return PredictionResult(
                intent='general_chat',
                confidence=0.5,
                entities={},
                response_template="Je n'ai pas bien compris, pouvez-vous reformuler ?"
            )
        
        entities = self.preprocessor.extract_entities(text)
        
        # Vectorisation
        X = self.vectorizer.transform([processed_text])
        
        # Pr√©diction
        try:
            predicted_label = self.best_model.predict(X)[0]
            
            # Confiance
            if hasattr(self.best_model, 'predict_proba'):
                probabilities = self.best_model.predict_proba(X)[0]
                confidence = probabilities.max()
            else:
                confidence = 0.8
            
            # D√©codage
            intent = self.label_encoder.inverse_transform([predicted_label])[0]
            
            return PredictionResult(
                intent=intent,
                confidence=confidence,
                entities=entities,
                response_template=f"Intent: {intent}"
            )
            
        except Exception as e:
            print(f"‚ùå Erreur pr√©diction: {e}")
            return PredictionResult(
                intent='general_chat',
                confidence=0.3,
                entities={},
                response_template="Erreur lors de l'analyse de votre demande."
            )

class RobustSemanticSearch:
    """Recherche s√©mantique """
    
    def __init__(self):
        print("üîç Initialisation recherche s√©mantique robuste...")
        
        self.model = None
        self.use_semantic = False
        
        # Essai de plusieurs mod√®les avec fallback
        models_to_try = [
            'paraphrase-multilingual-MiniLM-L12-v2',
            'distiluse-base-multilingual-cased',
            'all-MiniLM-L6-v2'
        ]
        
        for model_name in models_to_try:
            try:
                from sentence_transformers import SentenceTransformer
                self.model = SentenceTransformer(model_name)
                self.use_semantic = True
                print(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è  Tentative {model_name} √©chou√©e: {e}")
                continue
        
        if not self.use_semantic:
            print("üìù Mode recherche textuelle uniquement")
        
        self.product_embeddings = None
        self.products_df = None
        
    def index_products(self, products_data: List[Dict]):
        """Indexation robuste"""
        print("üîç Indexation des produits...")
        
        self.products_df = pd.DataFrame(products_data)
        
        if not self.use_semantic:
            print("üìù Indexation textuelle uniquement")
            return
        
        # Descriptions enrichies et mieux structur√©es
        descriptions = []
        for product in products_data:
            # Composition intelligente de la description
            parts = []
            
            # Nom et cat√©gorie (priorit√© haute)
            name = product.get('name', '')
            category = product.get('category', '')
            subcategory = product.get('subcategory', '')
            
            if name:
                parts.append(name)
            if category:
                parts.append(category)
            if subcategory:
                parts.append(subcategory)
            
            # Description
            description = product.get('description', '')
            if description:
                parts.append(description)
            
            # Mat√©riaux et couleurs
            materials = product.get('materials', [])
            colors = product.get('colors', [])
            occasions = product.get('occasions', [])
            
            if materials:
                parts.append(' '.join(materials))
            if colors:
                parts.append(' '.join(colors))
            if occasions:
                parts.append(' '.join(occasions))
            
            # Style et tags
            style_profile = product.get('style_profile', [])
            tags = product.get('tags', [])
            
            if style_profile:
                parts.append(' '.join(style_profile))
            if tags:
                parts.append(' '.join(tags))
            
            description = ' '.join(filter(None, parts))
            descriptions.append(description)
        
        try:
            if self.model:
                self.product_embeddings = self.model.encode(descriptions, show_progress_bar=True)
                print(f"   ‚úÖ {len(products_data)} produits index√©s avec embeddings")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur indexation s√©mantique: {e}")
            self.product_embeddings = None
            self.use_semantic = False
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """Recherche hybride robuste"""
        if self.products_df is None or len(self.products_df) == 0:
            return []
        
        # Recherche s√©mantique avec fallback robuste
        if self.use_semantic and self.product_embeddings is not None:
            try:
                return self._semantic_search(query, top_k)
            except Exception as e:
                print(f"‚ö†Ô∏è  Recherche s√©mantique √©chou√©e: {e}, fallback textuel")
                return self._enhanced_keyword_search(query, top_k)
        
        # Recherche textuelle am√©lior√©e
        return self._enhanced_keyword_search(query, top_k)
    
    def _semantic_search(self, query: str, top_k: int) -> List[Dict]:
        """Recherche s√©mantique robuste"""
        if not self.model:
            raise ValueError("Mod√®le s√©mantique non disponible")
        
        query_embedding = self.model.encode([query])
        
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(query_embedding, self.product_embeddings)[0]
        
        # Seuil adaptatif
        mean_similarity = similarities.mean()
        std_similarity = similarities.std()
        threshold = max(0.1, mean_similarity - 0.5 * std_similarity)
        
        # Top r√©sultats avec seuil intelligent
        top_indices = similarities.argsort()[-top_k*2:][::-1]  # Prend plus pour filtrer
        
        results = []
        for idx in top_indices:
            if similarities[idx] > threshold and len(results) < top_k:
                product = self.products_df.iloc[idx].to_dict()
                product['similarity_score'] = float(similarities[idx])
                results.append(product)
        
        return results
    
    def _enhanced_keyword_search(self, query: str, top_k: int) -> List[Dict]:
        """Recherche textuelle intelligente """
        query_lower = query.lower().strip()
        if not query_lower:
            return []
        
        results = []
        
        # Mots-cl√©s √©tendus et plus pr√©cis
        enhanced_keywords = {
            'sac': ['sac', 'sacoche', 'maroquinerie', 'cabas', 'pochette', 'besace', 'cartable'],
            'chaussure': ['chaussure', 'chaussures', 'escarpin', 'escarpins', 'ballerine', 'ballerines', 'soulier', 'souliers', 'chaussant'],
            'robe': ['robe', 'robes', 'dress'],
            'tailleur': ['tailleur', 'tailleurs', 'costume', 'suit', 'ensemble'],
            'bijou': ['bijou', 'bijoux', 'collier', 'colliers', 'bracelet', 'bague', 'perle', 'perles', 'jewelry'],
            'manteau': ['manteau', 'manteaux', 'veste', 'vestes', 'blouson', 'coat', 'jacket'],
            'vetement': ['vetement', 'vetements', 'habit', 'habits', 'tenue', 'pret-a-porter']
        }
        
        # Score plus nuanc√©
        for _, product in self.products_df.iterrows():
            score = 0
            product_dict = product.to_dict()
            
            # Texte du produit en minuscules
            product_texts = {
                'name': product_dict.get('name', '').lower(),
                'category': product_dict.get('category', '').lower(),
                'subcategory': product_dict.get('subcategory', '').lower(),
                'description': product_dict.get('description', '').lower(),
                'materials': ' '.join(product_dict.get('materials', [])).lower(),
                'colors': ' '.join(product_dict.get('colors', [])).lower(),
                'occasions': ' '.join(product_dict.get('occasions', [])).lower(),
                'tags': ' '.join(product_dict.get('tags', [])).lower()
            }
            
            all_product_text = ' '.join(product_texts.values())
            
            # Correspondance par cat√©gorie avec poids
            category_match_found = False
            for category, keywords in enhanced_keywords.items():
                if any(kw in query_lower for kw in keywords):
                    # V√©rifier correspondance dans les textes produit
                    for field, text in product_texts.items():
                        if any(kw in text for kw in keywords):
                            # Poids selon l'importance du champ
                            field_weights = {
                                'name': 15, 'category': 12, 'subcategory': 10,
                                'description': 8, 'tags': 6, 'materials': 4,
                                'colors': 3, 'occasions': 5
                            }
                            score += field_weights.get(field, 2)
                            category_match_found = True
            
            #  Correspondance de mots individuels
            query_words = [w for w in query_lower.split() if len(w) > 1]
            for word in query_words:
                if word in all_product_text:
                    # Bonus selon le contexte
                    if word in product_texts['name']:
                        score += 8
                    elif word in product_texts['category'] or word in product_texts['subcategory']:
                        score += 6
                    elif word in product_texts['description']:
                        score += 4
                    else:
                        score += 2
            
            # Bonus pour correspondance multi-mots
            if len(query_words) > 1:
                words_found = sum(1 for word in query_words if word in all_product_text)
                coverage = words_found / len(query_words)
                score += coverage * 5
            
            #  Bonus pour correspondance exacte d'expressions
            if len(query_lower) > 3 and query_lower in all_product_text:
                score += 10
            
            if score > 0:
                # Normalisation du score
                max_possible_score = 50  # Score th√©orique maximum
                normalized_score = min(score / max_possible_score, 1.0)
                product_dict['similarity_score'] = normalized_score
                results.append(product_dict)
        
        # Tri par score d√©croissant
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        return results[:top_k]

class OptimizedFashionChatbot:
    """Chatbot fashion ultra-optimis√© et fiable"""
    
    def __init__(self):
        self.intent_classifier = OptimizedIntentClassifier()
        self.semantic_search = RobustSemanticSearch()
        
    def load_training_data(self, training_file: str) -> Dict:
        """Chargement robuste des donn√©es"""
        try:
            with open(training_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úÖ Donn√©es d'entra√Ænement charg√©es: {len(data.get('training_data', []))} exemples")
            return data
        except Exception as e:
            print(f"‚ùå Erreur chargement training data: {e}")
            raise
    
    def load_products_data(self, products_file: str) -> Dict:
        """Chargement robuste des produits"""
        try:
            with open(products_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úÖ Donn√©es produits charg√©es: {len(data.get('products', []))} produits")
            return data
        except Exception as e:
            print(f"‚ùå Erreur chargement products data: {e}")
            raise
    
    def train_full_pipeline(self, training_file: str, products_file: str) -> Dict:
        print("‚ö° TF-IDF + Validation Crois√©e + Recherche hybride robuste")
        
        try:
            # Chargement des donn√©es
            training_data = self.load_training_data(training_file)
            products_data = self.load_products_data(products_file)
            
            # Validation des donn√©es
            if not training_data.get('training_data'):
                raise ValueError("Aucune donn√©e d'entra√Ænement trouv√©e!")
            if not products_data.get('products'):
                raise ValueError("Aucun produit trouv√©!")

            print("\nüî§ Construction du vocabulaire automatique...")
            self.intent_classifier.preprocessor.build_vocabulary_from_data(
                training_data['training_data']
            )
            
            # Entra√Ænement
            print("\nüìö Phase d'entra√Ænement...")
            training_results = self.intent_classifier.train(training_data['training_data'])
            
            # Indexation
            print("\nüîç Phase d'indexation...")
            self.semantic_search.index_products(products_data['products'])
            
            print("\n‚úÖ PIPELINE OPTIMIS√â PR√äT !")
            print(f"üéØ Pr√©cision CV: {training_results['cv_accuracy']:.1%}")
            print(f"üèÜ Meilleur mod√®le: {training_results['best_model']}")
            
            return {
                'intent_classification': training_results,
                'products_indexed': len(products_data['products']),
                'semantic_search_enabled': self.semantic_search.use_semantic,
                'status': 'optimized_success'
            }
            
        except Exception as e:
            print(f"‚ùå Erreur pipeline: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'intent_classification': None,
                'products_indexed': 0
            }

    def process_query(self, user_query: str) -> Dict:
        """Traitement de requ√™te ultra-robuste"""
        try:
            # Validation d'entr√©e
            if not user_query or not user_query.strip():
                return {
                    'success': True,
                    'intent': 'general_chat',
                    'confidence': 0.5,
                    'entities': {},
                    'recommended_products': [],
                    'response': "Bonjour ! Comment puis-je vous aider aujourd'hui ?"
                }
            
            # Classification avec gestion d'erreur
            try:
                prediction = self.intent_classifier.predict(user_query)
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur classification: {e}")
                return {
                    'success': False,
                    'error': f"Erreur classification: {e}",
                    'fallback_response': "D√©sol√©, je n'ai pas pu analyser votre demande. Pouvez-vous reformuler ?"
                }
            
            # Recherche produits selon l'intention
            products = []
            if prediction.intent in ['product_search', 'style_recommendation', 'stock_check', 'price_inquiry']:
                try:
                    products = self.semantic_search.search(user_query, top_k=3)
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur recherche: {e}")
                    products = []
            
            #  R√©ponse structur√©e et robuste
            response = self._generate_enhanced_response(prediction, products, user_query)
            
            return {
                'success': True,
                'intent': prediction.intent,
                'confidence': prediction.confidence,
                'entities': prediction.entities,
                'recommended_products': products,
                'response': response,
                'debug_info': {
                    'processed_query': self.intent_classifier.preprocessor.preprocess_for_ml(user_query),
                    'search_enabled': self.semantic_search.use_semantic,
                    'products_found': len(products)
                }
            }
            
        except Exception as e:
            print(f"‚ùå Erreur process_query: {e}")
            return {
                'success': False,
                'error': str(e),
                'fallback_response': "Une erreur technique s'est produite. Veuillez r√©essayer."
            }

    def _generate_enhanced_response(self, prediction: PredictionResult, products: List[Dict], query: str) -> str:
        """G√©n√©ration de r√©ponse intelligente et contextuelle - VERSION FINALE COMPL√àTE"""
        
        import random
        
        # Initialisation obligatoire de base_response
        base_response = ""
        
        # Templates de r√©ponses enrichis
        response_templates = {
            'general_chat': {
                'greetings': ['bonjour', 'salut', 'hello', 'bonsoir'],
                'thanks': ['merci', 'thank', 'remercie'],
                'goodbye': ['au revoir', 'bye', 'aurevoir', 'adieu'],
                'affirmative': ['oui', 'yes', 'parfait', 'ok', 'accord'],
                'negative': ['non', 'no', 'pas'],
                'polite': ['excusez', 'pardon', 'desolee', 'sil vous plait']
            },
            'product_search': [
                "Voici notre s√©lection exclusive qui pourrait vous int√©resser :",
                "J'ai trouv√© ces cr√©ations Chanel parfaites pour vous :",
                "D√©couvrez ces pi√®ces iconiques de notre collection :"
            ],
            'price_inquiry': [
                "Voici les informations tarifaires demand√©es :",
                "Nos prix pour ces cr√©ations :",
                "Budget n√©cessaire pour ces articles :"
            ],
            'stock_check': [
                "√âtat actuel de nos stocks :",
                "Disponibilit√© en temps r√©el :",
                "V√©rification de notre inventaire :"
            ],
            'style_recommendation': [
                "Mes conseils de style personnalis√©s :",
                "Je recommande pour votre occasion :",
                "L'alliance parfaite pour vous :"
            ],
            'care_instructions': [
                "Conseils d'entretien de nos experts :",
                "Pour pr√©server vos cr√©ations Chanel :",
                "Instructions de soin recommand√©es :"
            ]
        }
        
        # D√©tection contextuelle
        query_lower = query.lower()
        
        if prediction.intent == 'general_chat':
            # D√©tection fine du contexte
            context_detected = False
            
            for context, keywords in response_templates['general_chat'].items():
                if any(kw in query_lower for kw in keywords):
                    if context == 'greetings':
                        base_response = "Bonjour et bienvenue chez Chanel Maison Heritage ! Comment puis-je vous accompagner dans votre recherche ?"
                    elif context == 'thanks':
                        base_response = "Je vous en prie ! C'est un plaisir de vous conseiller. Puis-je vous aider avec autre chose ?"
                    elif context == 'goodbye':
                        base_response = "Au revoir et merci de votre visite ! N'h√©sitez pas √† revenir pour d√©couvrir nos nouvelles cr√©ations."
                    elif context == 'affirmative':
                        base_response = "Parfait ! Comment puis-je continuer √† vous assister ?"
                    elif context == 'negative':
                        base_response = "D'accord, puis-je vous proposer autre chose ou vous renseigner diff√©remment ?"
                    elif context == 'polite':
                        base_response = "Aucun souci ! Je suis l√† pour vous aider. Que puis-je faire pour vous ?"
                    context_detected = True
                    break
            
            if not context_detected:
                base_response = "Comment puis-je vous accompagner dans votre d√©couverte de nos cr√©ations ?"
        
        elif prediction.intent == 'care_instructions':
            # Instructions d'entretien d√©taill√©es et sp√©cifiques
            care_instructions = {
                'cuir': """
    **Entretien du cuir d'agneau Chanel :**
    ‚Ä¢ Nettoyage : Utilisez un chiffon doux l√©g√®rement humide
    ‚Ä¢ Protection : Appliquez un produit hydratant sp√©cial cuir tous les 3 mois
    ‚Ä¢ Stockage : Conservez dans un dustbag √† l'abri de la lumi√®re
    ‚Ä¢ √âvitez : L'eau, la chaleur directe et les produits chimiques
    ‚Ä¢ En cas de tache : Consultez imm√©diatement nos experts en boutique""",
                
                'chaussures': """
    **Entretien sp√©cialis√© pour chaussures Chanel :**
    ‚Ä¢ Nettoyage quotidien : Brossage d√©licat avec brosse en crin
    ‚Ä¢ Cirage : Utilisez un cirage de qualit√© adapt√© √† la couleur
    ‚Ä¢ Embauchoirs : Ins√©rez des embauchoirs en bois apr√®s chaque port
    ‚Ä¢ Rotation : Alternez vos paires pour laisser respirer le cuir
    ‚Ä¢ R√©paration : Faites ressemeler chez un cordonnier sp√©cialis√©
    ‚Ä¢ Stockage : Conservez dans leur bo√Æte d'origine avec papier de soie""",
                
                'sac': """
    **Entretien des sacs en cuir Chanel :**
    ‚Ä¢ Nettoyage mensuel avec un chiffon microfibre l√©g√®rement humide
    ‚Ä¢ Conditionnement du cuir tous les 3-4 mois avec un produit sp√©cialis√©
    ‚Ä¢ Stockage dans le dustbag fourni, rempli de papier pour garder la forme
    ‚Ä¢ √âvitez la surcharge pour pr√©server la structure
    ‚Ä¢ Rotation des anses pour √©viter l'usure in√©gale""",
                
                'tweed': """
    **Entretien du tweed Chanel :**
    ‚Ä¢ Nettoyage √† sec uniquement chez un sp√©cialiste textiles de luxe
    ‚Ä¢ Stockage sur cintre large pour pr√©server la forme
    ‚Ä¢ Brossage d√©licat dans le sens du tissu avec une brosse √† v√™tements
    ‚Ä¢ Protection anti-mites avec sachets naturels (lavande, c√®dre)
    ‚Ä¢ √âvitez l'exposition prolong√©e au soleil et √† l'humidit√©""",
                
                'soie': """
    **Entretien de la soie :**
    ‚Ä¢ Nettoyage √† sec professionnel obligatoire
    ‚Ä¢ Repassage √† temp√©rature douce (110¬∞C max) avec un linge de protection
    ‚Ä¢ Stockage suspendu dans un endroit sec et a√©r√©
    ‚Ä¢ √âvitez les parfums, d√©odorants et produits acides""",
                
                'perles': """
    **Entretien des perles de culture :**
    ‚Ä¢ Nettoyage apr√®s chaque port avec un chiffon doux et sec
    ‚Ä¢ √âvitez absolument le contact avec parfums, cosm√©tiques et transpiration
    ‚Ä¢ Stockage s√©par√© des autres bijoux dans une pochette douce
    ‚Ä¢ Enfilage √† v√©rifier annuellement chez un bijoutier"""
            }
            
            # S√©lection du template de base
            templates = response_templates.get('care_instructions', ["Conseils d'entretien :"])
            base_response = random.choice(templates)
            
            # D√©tection intelligente du mat√©riau/produit
            material_found = None
            
            if any(word in query_lower for word in ['chaussure', 'chaussures', 'escarpin', 'ballerine', 'soulier']):
                material_found = 'chaussures'
            elif any(word in query_lower for word in ['sac', 'sacoche', 'maroquinerie']):
                material_found = 'sac'
            elif 'cuir' in query_lower:
                material_found = 'cuir'
            elif 'tweed' in query_lower:
                material_found = 'tweed'
            elif any(word in query_lower for word in ['soie', 'satin']):
                material_found = 'soie'
            elif any(word in query_lower for word in ['perle', 'perles', 'bijou', 'collier']):
                material_found = 'perles'
            
            if material_found and material_found in care_instructions:
                base_response += care_instructions[material_found]
            else:
                base_response += """
    **Conseils d'entretien g√©n√©raux pour vos cr√©ations Chanel :**
    ‚Ä¢ Suivez toujours les √©tiquettes d'entretien sp√©cifiques
    ‚Ä¢ Privil√©giez le nettoyage professionnel pour les pi√®ces d√©licates
    ‚Ä¢ Stockez vos articles dans leurs dustbags d'origine
    ‚Ä¢ √âvitez l'exposition directe √† la lumi√®re et √† la chaleur
    ‚Ä¢ Consultez nos conseillers pour des instructions personnalis√©es"""
        
        elif prediction.intent == 'stock_check' and not products:
            # S√©lection du template de base
            templates = response_templates.get('stock_check', ["√âtat de nos stocks :"])
            base_response = random.choice(templates)
            
            # Gestion compl√®te des stocks par localisation
            location_stocks = {
                'paris': {
                    'name': 'Paris Cambon',
                    'products': [
                        'Tailleur Tweed Iconique : 5 pi√®ces',
                        'Robe Satin Asym√©trique : 4 pi√®ces', 
                        'Sac Matelass√© Cha√Æne Dor√©e : 6 pi√®ces',
                        'Escarpins Bout Rond : 8 pi√®ces',
                        'Collier Perles Signature : 8 pi√®ces',
                        'Ballerines Cuir Nude : 10 pi√®ces',
                        'Manteau Boucl√© Long : 3 pi√®ces',
                        'Sac Cabas Grand Format : 4 pi√®ces'
                    ]
                },
                'lyon': {
                    'name': 'Lyon Presqu\'√Æle',
                    'products': [
                        'Tailleur Tweed Iconique : 3 pi√®ces',
                        'Robe Satin Asym√©trique : 3 pi√®ces',
                        'Sac Matelass√© Cha√Æne Dor√©e : 4 pi√®ces',
                        'Escarpins Bout Rond : 6 pi√®ces',
                        'Collier Perles Signature : 5 pi√®ces',
                        'Ballerines Cuir Nude : 8 pi√®ces',
                        'Manteau Boucl√© Long : 2 pi√®ces',
                        'Sac Cabas Grand Format : 3 pi√®ces'
                    ]
                },
                'cannes': {
                    'name': 'Cannes Croisette',
                    'products': [
                        'Robe Satin Asym√©trique : 3 pi√®ces',
                        'Sac Matelass√© Cha√Æne Dor√©e : 5 pi√®ces',
                        'Escarpins Bout Rond : 4 pi√®ces',
                        'Collier Perles Signature : 6 pi√®ces',
                        'Ballerines Cuir Nude : 6 pi√®ces',
                        'Tailleur Tweed Iconique : 2 pi√®ces',
                        'Manteau Boucl√© Long : 1 pi√®ce'
                    ]
                }
            }
            
            detected_location = None
            for loc_key in location_stocks.keys():
                if loc_key in query_lower:
                    detected_location = loc_key
                    break
            
            if detected_location:
                stock_info = location_stocks[detected_location]
                base_response += f"""
    **Stock disponible √† {stock_info['name']} :**

    Voici notre inventaire complet dans cette boutique :
    """
                for product in stock_info['products']:
                    base_response += f"‚Ä¢ {product}\n"
                
                base_response += "\nüìû Pour r√©server un article ou obtenir plus d'informations, contactez directement la boutique."
            else:
                base_response += """
    **Nos stocks sont mis √† jour en temps r√©el dans nos 3 boutiques :**

    ‚Ä¢ **Paris Cambon** : Notre flagship avec la collection compl√®te
    ‚Ä¢ **Lyon Presqu'√Æle** : S√©lection premium et nouveaut√©s  
    ‚Ä¢ **Cannes Croisette** : Collection saisonni√®re et pi√®ces exclusives

    üí° Pr√©cisez une boutique (Paris, Lyon ou Cannes) pour voir le d√©tail des stocks."""
        
        else:
            # Pour toutes les autres intentions, utiliser les templates
            templates = response_templates.get(prediction.intent, ["Voici ce que j'ai trouv√© pour vous :"])
            base_response = random.choice(templates)
        
        # Affichage enrichi des produits
        if products and prediction.intent in ['product_search', 'price_inquiry', 'style_recommendation']:
            base_response += "\n"
            
            for i, product in enumerate(products[:3], 1):
                # Informations essentielles
                name = product.get('name', 'Produit')
                price = product.get('price', 0)
                currency = product.get('currency', 'EUR')
                
                # Informations contextuelles
                materials = product.get('materials', [])
                colors = product.get('colors', [])
                occasions = product.get('occasions', [])
                
                # Score de pertinence
                similarity = product.get('similarity_score', 0)
                
                # Construction de la description
                product_line = f"\n{i}. **{name}** - {price} {currency}"
                
                # Ajout d'informations contextuelles
                details = []
                if materials:
                    details.append(f"Mati√®res: {', '.join(materials[:2])}")
                if colors:
                    details.append(f"Couleurs: {', '.join(colors[:2])}")
                if occasions:
                    details.append(f"Occasions: {', '.join(occasions[:2])}")
                
                if details:
                    product_line += f"\n   üìù {' ‚Ä¢ '.join(details)}"
                
                # Indicateur de pertinence
                if similarity > 0.7:
                    product_line += " ‚≠ê *Excellent match*"
                elif similarity > 0.5:
                    product_line += " ‚ú® *Bon match*"
                
                base_response += product_line
        
        elif prediction.intent in ['product_search', 'style_recommendation'] and not products:
            # Cas o√π aucun produit n'est trouv√©
            base_response += "\n\nJe n'ai pas trouv√© de produit correspondant exactement √† votre recherche."
            base_response += "\nPourriez-vous pr√©ciser vos pr√©f√©rences (couleur, occasion, budget) ?"
        
        # Informations contextuelles enrichies
        if prediction.entities:
            context_info = []
            
            # Entit√©s importantes √† mettre en valeur
            priority_entities = ['color', 'occasion', 'material', 'product_type', 'price']
            
            for entity_type in priority_entities:
                if entity_type in prediction.entities:
                    values = prediction.entities[entity_type]
                    entity_display = {
                        'color': 'üé® Couleur',
                        'occasion': '‚ú® Occasion', 
                        'material': 'üßµ Mati√®re',
                        'product_type': 'üëú Type',
                        'price': 'üí∞ Budget'
                    }
                    
                    display_name = entity_display.get(entity_type, entity_type.title())
                    context_info.append(f"{display_name}: {', '.join(values)}")
            
            if context_info:
                base_response += f"\n\nüîç **Crit√®res d√©tect√©s:** {' ‚Ä¢ '.join(context_info)}"
        
        # Suggestions proactives
        if prediction.intent == 'product_search' and products:
            base_response += "\n\nüí° *Conseil: Dites-moi \"plus d'infos sur le produit 1\" pour obtenir des d√©tails complets.*"
        elif prediction.intent == 'style_recommendation':
            base_response += "\n\nüëó *Astuce: Je peux aussi vous conseiller des accessoires assortis !*"
        elif prediction.intent == 'stock_check' and products:
            # Informations de stock si disponibles
            stock_info = []
            for product in products[:2]:
                stock_locations = product.get('stock_locations', {})
                if stock_locations:
                    total_stock = sum(stock_locations.values())
                    if total_stock > 0:
                        stock_info.append(f"{product['name']}: {total_stock} pi√®ces disponibles")
                    else:
                        stock_info.append(f"{product['name']}: Sur commande")
            
            if stock_info:
                base_response += f"\n\nüì¶ **Stock:** {' ‚Ä¢ '.join(stock_info)}"
        
        # Appel √† l'action minimal et discret
        confidence = prediction.confidence
    
        if confidence > 0.8:
            base_response += "\n\n‚ùì Y a-t-il autre chose que je puisse vous proposer ?"
        
        return base_response

    def save_model(self, filepath: str):
        """Sauvegarde robuste du mod√®le"""
        try:
            import pickle
            model_data = {
                'intent_classifier': self.intent_classifier,
                'vectorizer': self.intent_classifier.vectorizer,
                'label_encoder': self.intent_classifier.label_encoder,
                'products_df': self.semantic_search.products_df,
                'use_semantic': self.semantic_search.use_semantic
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"‚úÖ Mod√®le sauvegard√©: {filepath}")
        except Exception as e:
            print(f"‚ùå Erreur sauvegarde: {e}")
    
    def load_model(self, filepath: str):
        """Chargement robuste du mod√®le"""
        try:
            import pickle
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.intent_classifier = model_data['intent_classifier']
            self.semantic_search.products_df = model_data['products_df']
            self.semantic_search.use_semantic = model_data.get('use_semantic', False)
            
            print(f"‚úÖ Mod√®le charg√©: {filepath}")
        except Exception as e:
            print(f"‚ùå Erreur chargement: {e}")
            raise

# Test principal 
if __name__ == "__main__":
    print("üéØ Validation crois√©e + Recherche hybride + Gestion d'erreurs")
    
    chatbot = OptimizedFashionChatbot()
    
    try:
        # Entra√Ænement avec gestion d'erreur robuste
        print("\nüìö Phase d'entra√Ænement...")
        results = chatbot.train_full_pipeline(
            'training_data.json',  # Chemins relatifs
            'products.json'
        )
        
        if results['status'] == 'optimized_success':
            print(f"\nüéâ ENTRA√éNEMENT R√âUSSI !")
            print(f"üéØ Pr√©cision CV: {results['intent_classification']['cv_accuracy']:.1%}")
            print(f"üîç Produits index√©s: {results['products_indexed']}")
            print(f"üîÆ Recherche s√©mantique: {'‚úÖ Activ√©e' if results['semantic_search_enabled'] else 'üìù Textuelle uniquement'}")
            
            # Tests vari√©s avec gestion d'erreur
            test_queries = [
                "Bonjour !",
                "",  # Test cas vide
                "Je cherche un sac noir pour le bureau",
                "Avez-vous des chaussures confortables ?",
                "Combien co√ªte le tailleur tweed ?",
                "Stock disponible √† Paris ?",
                "Que me conseillez-vous pour un mariage ?",
                "Comment entretenir le cuir d'agneau ?",
                "sdkfjhsdkjfh",  # Test cas incompr√©hensible
                "Merci beaucoup, au revoir !",
                "Des bijoux en perles ?"
            ]
            
            print("\nüß™ TESTS VARI√âS:")
            for i, query in enumerate(test_queries, 1):
                print(f"\n--- Test {i} ---")
                result = chatbot.process_query(query)
                
                if result['success']:
                    print(f"üí¨ '{query}'")
                    print(f"üéØ Intent: {result['intent']} (confiance: {result['confidence']:.1%})")
                    
                    if result['entities']:
                        print(f"üè∑Ô∏è  Entit√©s: {result['entities']}")
                    
                    if result['recommended_products']:
                        print(f"üõçÔ∏è  Produits trouv√©s: {len(result['recommended_products'])}")
                    
                    # R√©ponse tronqu√©e pour l'affichage
                    response_preview = result['response'][:100]
                    if len(result['response']) > 100:
                        response_preview += "..."
                    print(f"üí° R√©ponse: {response_preview}")
                    
                    # Debug info
                    debug = result.get('debug_info', {})
                    print(f"üîß Debug: Query process√©e='{debug.get('processed_query', 'N/A')[:30]}...', Produits={debug.get('products_found', 0)}")
                else:
                    print(f"‚ùå Erreur: {result.get('error', 'Inconnue')}")
                    print(f"üîÑ Fallback: {result.get('fallback_response', 'Aucun')}")
            
            # Sauvegarde du mod√®le
            print(f"\nüíæ Sauvegarde du mod√®le...")
            chatbot.save_model('fashion_chatbot_optimized.pkl')
            
        else:
            print(f"‚ùå √âCHEC ENTRA√éNEMENT: {results.get('error', 'Erreur inconnue')}")
            
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©rale: {e}")
        import traceback
        traceback.print_exc()